{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/che.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompanyNo</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>che</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Rampart Division</td>\n",
       "      <td>킹고인 창업경진대회 &lt;장려상&gt; 성균관대학교 캠퍼스타운\\n창업아이템 검증프로그램 &lt;선...</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>술며든다</td>\n",
       "      <td>초기창업패키지 킹고창업클럽\\n제4차 심화형 일반인 창업강좌\\n포스코 창업 인큐베이팅...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>라이더타운</td>\n",
       "      <td>창업의 이해\\n디자인 씽킹 실습\\n창업 아이템 구체화, 고객반응 확인\\n스타트업 시...</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>FIT ME</td>\n",
       "      <td>제 13특수임무여단 부중대장 특전사 임무 수행\\nYOUFIT 매니저 회원 운동처방 ...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53</td>\n",
       "      <td>지중해</td>\n",
       "      <td>스타벅스 창업카페 -인사이트 공유 프로그램- 초기창업 시 주의점</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CompanyNo       CompanyName  \\\n",
       "0          1  Rampart Division   \n",
       "1          8              술며든다   \n",
       "2         12             라이더타운   \n",
       "3         13            FIT ME   \n",
       "4         53               지중해   \n",
       "\n",
       "                                                 che label  \n",
       "0  킹고인 창업경진대회 <장려상> 성균관대학교 캠퍼스타운\\n창업아이템 검증프로그램 <선...     B  \n",
       "1  초기창업패키지 킹고창업클럽\\n제4차 심화형 일반인 창업강좌\\n포스코 창업 인큐베이팅...     C  \n",
       "2  창업의 이해\\n디자인 씽킹 실습\\n창업 아이템 구체화, 고객반응 확인\\n스타트업 시...     B  \n",
       "3  제 13특수임무여단 부중대장 특전사 임무 수행\\nYOUFIT 매니저 회원 운동처방 ...     A  \n",
       "4                스타벅스 창업카페 -인사이트 공유 프로그램- 초기창업 시 주의점     C  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonnlp as nlp\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import torch.nn as nn \n",
    "import torch \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob \n",
    "\n",
    "from utils import * \n",
    "from settings import *\n",
    "import argparse, os\n",
    "import numpy as np \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import AdamW\n",
    "\n",
    "import gluonnlp as nlp\n",
    "from kobert import get_pytorch_kobert_model, get_tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import * \n",
    "from settings import * \n",
    "from bert_utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonnlp as nlp\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import torch.nn as nn \n",
    "import torch \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob \n",
    "\n",
    "from utils import * \n",
    "from settings import *\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, X:pd.Series, Y:pd.Series, tokenizer, max_len, pad=True, pair=False):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: sentences\n",
    "        Y: label \n",
    "\n",
    "\n",
    "        df : dataframe (trainset or testset)\n",
    "        X : sentences -> str\n",
    "        Y : label -> int\n",
    "        '''\n",
    "        self.tokenizer = tokenizer \n",
    "        self.max_len = max_len \n",
    "        self.pad = pad \n",
    "        self.pair = pair\n",
    "\n",
    "        self.transform = nlp.data.BERTSentenceTransform(tokenizer, max_seq_length=self.max_len, pad=self.pad, pair=self.pair)\n",
    "\n",
    "        self.X = X # sentence\n",
    "        self.Y = Y # label\n",
    "        self.sentences = [self.transform(x) for x in self.X]\n",
    "        self.labels = [np.int8(y) for y in self.Y]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.sentences[i] + (self.labels[i], )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, model, hidden_size, num_classes, dr_rate):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.model = model \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.dr_rate = dr_rate\n",
    "\n",
    "        self.classifier = nn.Linear(self.hidden_size, self.num_classes)\n",
    "\n",
    "        if self.dr_rate: \n",
    "            self.dropout = nn.Dropout(p=self.dr_rate)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        _, pooler = self.model(input_ids=token_ids, token_type_ids=segment_ids, attention_mask=attention_mask.float())\n",
    "        \n",
    "        if self.dr_rate: \n",
    "            pooler = self.dropout(pooler)\n",
    "\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "def get_file_info(get_file_name = False):\n",
    "    '''\n",
    "    Channels.csv\n",
    "    CostStructure.csv\n",
    "    CustomerRelationships.csv\n",
    "    CustomerSegmentations.csv\n",
    "    KeyActivities.csv\n",
    "    KeyPartners.csv\n",
    "    KeyResources.csv \n",
    "    RevenueStreams.csv\n",
    "    ValuePropositions.csv\n",
    "    '''\n",
    "    file_path = glob.glob(f'{BASE_DIR}/data/*.csv') # concatenate dataset (it, manufacture)\n",
    "    return sorted(file_path)\n",
    "\n",
    "def label_encoder(data, reverse=False):\n",
    "    label_dic = {\n",
    "        'A' : 0,\n",
    "        'B' : 1, \n",
    "        'C' : 2\n",
    "    }\n",
    "    if reverse:\n",
    "        label_dic = {value: key for key, value in label_dic.items()}\n",
    "    return label_dic[data]\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing(file_path):\n",
    "    # label encoding [ e.g. A -> 0, B -> 1]\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    df.iloc[:, -1] = df.iloc[:, -1].apply(lambda x: label_encoder(x) if x in ['A', 'B', 'C'] else np.NaN)\n",
    "    df.dropna(inplace=True, axis=0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82347/3383164954.py:1: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, -1] = df.iloc[:, -1].apply(lambda x: label_encoder(x) if x in ['A', 'B', 'C'] else np.NaN)\n"
     ]
    }
   ],
   "source": [
    "df.iloc[:, -1] = df.iloc[:, -1].apply(lambda x: label_encoder(x) if x in ['A', 'B', 'C'] else np.NaN)\n",
    "df.dropna(inplace=True, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/bax/문서/20221212test/.cache/kobert_v1.zip\n",
      "using cached model. /home/bax/문서/20221212test/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
      "using cached model. /home/bax/문서/20221212test/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "bert_models, vocab = get_pytorch_kobert_model()\n",
    "bert_tokenizer = get_tokenizer()\n",
    "tokenizer = nlp.data.BERTSPTokenizer(bert_tokenizer, vocab, lower=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=1028\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = BERTDataset(df.iloc[:, -2], df.iloc[:, -1], tokenizer=tokenizer, max_len=max_len)\n",
    "train_loader = DataLoader(train, batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f3c65940c40>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('mx')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Dec  7 2022, 01:12:08) \n[GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b90fb7282f776f0fb6f59761b3f05669fb074d43fcf656907b74edfc7af4ec69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
